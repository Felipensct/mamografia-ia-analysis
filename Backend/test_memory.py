#!/usr/bin/env python3
"""
Script para testar o lazy loading do modelo
"""

import os
import time
from services.model_service import get_model_service

def test_model_lazy_loading():
    print("="*70)
    print("TESTE DE LAZY LOADING DO MODELO")
    print("="*70)
    
    # Criar servi√ßo (sem carregar modelo)
    print("\n1Ô∏è‚É£ Criando ModelService (lazy loading)...")
    model_service = get_model_service()
    print(f"   ‚úÖ ModelService criado")
    print(f"   üìã Modelo carregado na mem√≥ria? {model_service.model is not None}")
    
    # Verificar disponibilidade
    is_available = model_service.is_available()
    print(f"\n2Ô∏è‚É£ Modelo dispon√≠vel (arquivo existe)? {is_available}")
    
    # Fazer predi√ß√£o (carrega modelo)
    print("\n3Ô∏è‚É£ Fazendo predi√ß√£o (modelo ser√° carregado AGORA)...")
    test_image = "/home/frog/ai/jpeg/1.3.6.1.4.1.9590.100.1.2.499558611862523307025745211397332529/1-036.jpg"
    
    if os.path.exists(test_image):
        print(f"   üñºÔ∏è Usando imagem: {os.path.basename(test_image)}")
        result = model_service.predict(test_image, generate_viz=False)
        
        if result.get('success'):
            print(f"   ‚úÖ Resultado: {result.get('prediction', 'N/A')} ({result.get('probability', 0):.1%})")
            print(f"   üìã Modelo ainda na mem√≥ria ap√≥s predi√ß√£o? {model_service.model is not None}")
        else:
            print(f"   ‚ùå Erro: {result.get('error', 'Unknown')}")
    else:
        print(f"   ‚ö†Ô∏è Imagem de teste n√£o encontrada: {test_image}")
    
    # Resumo
    print("\n" + "="*70)
    print("RESUMO DO LAZY LOADING")
    print("="*70)
    print(f"‚úÖ Modelo N√ÉO √© carregado no __init__ (economiza mem√≥ria)")
    print(f"‚úÖ Modelo √© carregado sob demanda quando predict() √© chamado")
    print(f"‚úÖ Modelo √© descarregado automaticamente ap√≥s cada predi√ß√£o")
    print(f"‚úÖ Mem√≥ria √© liberada com garbage collection")
    print("="*70)

if __name__ == "__main__":
    test_model_lazy_loading()
